üìö Books to Scrape RESTful API

## Vis√£o Geral do Projeto

Este projeto consiste em uma **API RESTful de alto desempenho** constru√≠da com **FastAPI** para consultar, analisar e atualizar um *dataset* de livros extra√≠do do site `books.toscrape.com`.

O projeto √© um exemplo robusto de como integrar o **Web Scraping** (utilizando `requests` e `BeautifulSoup` para performance) com uma API ass√≠ncrona. A tarefa de *scraping* √© isolada em um *thread* de *background* (`BackgroundTasks`), garantindo que o servidor permane√ßa responsivo em todos os momentos. Os dados s√£o carregados em mem√≥ria (`utils.py`) e disponibilizados atrav√©s de *routers* especializados (Livros, Categorias, Estat√≠sticas).

> üèÜ **Projeto:** Primeiro Tech Challenge de R√¥mulo Carri√ßo

-----

## üèóÔ∏è Arquitetura e Estrutura de Arquivos

A aplica√ß√£o adota uma arquitetura modular com *routers* separados para cada dom√≠nio, facilitando a manuten√ß√£o e o escalonamento.

```
.
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # Ponto de entrada da aplica√ß√£o FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ auth.py          # Roteador de Autentica√ß√£o (Login, Refresh Token)
‚îÇ   ‚îú‚îÄ‚îÄ books.py         # Roteador de Livros (Busca, Filtro, Detalhes)
‚îÇ   ‚îú‚îÄ‚îÄ categories.py    # Roteador de Categorias (Lista √∫nica)
‚îÇ   ‚îú‚îÄ‚îÄ health.py        # Roteador de Sa√∫de (API e Dados)
‚îÇ   ‚îú‚îÄ‚îÄ scrap.py         # Roteador de Orquestra√ß√£o do Web Scraping
‚îÇ   ‚îî‚îÄ‚îÄ stats.py         # Roteador para Estat√≠sticas (Pandas)
‚îú‚îÄ‚îÄ Script/
‚îÇ   ‚îî‚îÄ‚îÄ WebScrap.py      # L√≥gica de Web Scraping (Requests + BeautifulSoup + Cache)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ Livros.csv       # Arquivo de dados principal
‚îú‚îÄ‚îÄ models.py            # Defini√ß√£o de Schemas Pydantic
‚îú‚îÄ‚îÄ utils.py             # Fun√ß√µes utilit√°rias (Carregamento e pr√©-processamento de dados)
‚îú‚îÄ‚îÄ auth_utils.py        # Fun√ß√µes de JWT (Cria√ß√£o/Verifica√ß√£o de Token, Seguran√ßa HTTP Bearer)
‚îî‚îÄ‚îÄ requirements.txt     # Depend√™ncias do projeto
```

-----

## ‚öôÔ∏è Tecnologias e Configura√ß√£o

### 1\. Depend√™ncias Fixadas (`requirements.txt`)

O arquivo `requirements.txt` lista todas as depend√™ncias com vers√µes fixadas para garantir a reprodutibilidade, essencial para ambientes de *deploy* (como o Render).

### 2\. Configura√ß√£o de Autentica√ß√£o

O m√≥dulo `auth_utils.py` √© respons√°vel pela seguran√ßa, implementando:

  * Gera√ß√£o e decodifica√ß√£o de JWTs (HS256).
  * Tokens de **Acesso** (curta dura√ß√£o) e **Refresh** (longa dura√ß√£o) para melhor seguran√ßa e usabilidade.
  * Esquema de seguran√ßa **HTTP Bearer** (`get_current_user`) para proteger rotas.

> **Importante:** A verifica√ß√£o de senha √© simplificada (`verify_password_simple`).

### 3\. Instala√ß√£o e Execu√ß√£o

1.  **Instala√ß√£o:**

    ```bash
    pip install -r requirements.txt
    ```

2.  **Execu√ß√£o Local:**

    ```bash
    uvicorn main:app --reload
    ```

    (Acesse a documenta√ß√£o interativa em `http://127.0.0.1:8000/docs`)

-----

## üíª Endpoints da API

Todos os endpoints est√£o prefixados com `/api/v1`.

### A. Web Scraping & Dados (`api/scrap.py` e `api/health.py`)

| M√©todo | Endpoint | Resumo |
| :--- | :--- | :--- |
| `POST` | `/v1/scrap` | **Inicia o Web Scraping.** Executado em *BackgroundTasks* (`202 Accepted`). **Protegido.** |
| `GET` | `/v1/scrap_status`| Verifica o *status* da tarefa de *scraping*. |
| `GET` | `/v1/health` | Verifica a sa√∫de da API (`online`) e do *dataset* (`ok`/`erro`). |

### B. Autentica√ß√£o (`api/auth.py`)

| M√©todo | Endpoint | Resumo |
| :--- | :--- | :--- |
| `POST` | `/v1/auth/login` | Obt√©m `Access Token` e `Refresh Token`. |
| `POST` | `/v1/auth/refresh` | Renova o `Access Token` usando o `Refresh Token`. |

### C. Consultas aos Livros (`api/books.py` e `api/categories.py`)

| M√©todo | Endpoint | Resumo |
| :--- | :--- | :--- |
| `GET` | `/v1/books/` | Lista todos os livros. |
| `GET` | `/v1/books/search`| Busca por `title` e/ou `category`. |
| `GET` | `/v1/books/price_range`| Filtra por faixa de pre√ßo (`min` e `max`). |
| `GET` | `/v1/books/{book_id}` | Retorna detalhes de um livro por ID. |
| `GET` | `/v1/categories` | Lista todas as categorias √∫nicas, ordenadas alfabeticamente. |

### D. An√°lise de Dados (`api/stats.py`)

O m√≥dulo `stats.py` utiliza **Pandas** para realizar c√°lculos complexos em tempo real a partir dos dados carregados em mem√≥ria.

| M√©todo | Endpoint | Resumo |
| :--- | :--- | :--- |
| `GET` | `/v1/stats/overview` | Estat√≠sticas gerais (Total, Pre√ßo M√©dio, Distribui√ß√£o de Ratings 1-5). |
| `GET` | `/v1/stats/categories`| Estat√≠sticas por categoria (Contagem e Pre√ßo M√©dio). |

-----

## üí° Mecanismos Chave do Projeto

### 1\. Carregamento de Dados em Mem√≥ria (`utils.py`)

  * A fun√ß√£o `utils.load_data()` √© executada uma vez na inicializa√ß√£o da API.
  * L√™ o arquivo `Livros.csv`, normaliza os cabe√ßalhos, converte tipos (`preco` para float, `rating` para int) e **adiciona um `id` sequencial** (coluna zero).
  * Os dados pr√©-processados s√£o armazenados na vari√°vel global `BOOKS_DATA`, eliminando a lat√™ncia de I/O em cada requisi√ß√£o de leitura.

### 2\. Fluxo Otimizado de Web Scraping (`WebScrap.py`)

O script de *scraping* foi otimizado para velocidade:

  * **Requests & BeautifulSoup:** Conjunto r√°pido e seguro para extra√ß√£o dos dados.
  * **Cache:** Implementa um *cache* de **5 minutos** para evitar requisi√ß√µes desnecess√°rias, reutilizando o CSV mais recente.
  * **Rate Limiting:** Adiciona `time.sleep(0.1)` por livro para mitigar o risco de bloqueio pelo servidor.

### 3\. Modela√ß√£o Robusta (`models.py`)

Todos os dados de entrada e sa√≠da s√£o rigorosamente tipados e validados pelo Pydantic, garantindo:

  * Clareza na documenta√ß√£o (Swagger UI).
  * Valida√ß√£o autom√°tica de dados (ex: `preco` como `float`, `rating` como `int`).
  * Modelos espec√≠ficos para respostas estat√≠sticas (`OverviewStats`, `CategoryStats`), garantindo que o formato JSON retornado seja previs√≠vel.
